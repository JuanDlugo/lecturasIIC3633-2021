Partamos notando lo que analiza el texto pedido esta semana. El documento analiza los sistemas de recomendación basados en contenido, es decir, los sistemas que recomiendan un item a un usuario en función de una descripción del item y un perfil de los intereses del usuario.

Uno de los primeros temas que toca el artículo es la difencia entre la data estructurada y la no estructurada, la cual me pareció muy interante. Cuando tenemos data estructurada (numeros, vectores, valores) es fácil ordenar esta información para entregarsela al sistema recomendador o a algún modelo de IA. El problema es que hacemos con data no estructurada, datos como textos, poemas y lenguajes naturales en los cuales no hay un orden ordinal o cardinal que nos diferencie y note la distancia entre 2 items.
¿Cómo modelamos información o data no estructurada, de manera que pueda ser procesada por un modelo recomendador?
Generalmente se ocupa algún que otro método para representar cada palabra del texto de una forma estructurada como por ejemplo un vector (i.e. one-hot encoding).
En el artículo mencionan la técnica llamada stemming que consiste en calcular la importancia que tiene cada palabra en el texto. Esto se realiza mediante sus repeticiones en el texto y se normalizan según la cantidad de palabras totales. Lo interesante de esta técnica es que nos permite traspasar nuestro texto subjetivo a un espacio vectorial objetivo. Por otro lado el problema que surge es que si bien sabemos cuales son las palabras más relevantes del texto y tenemos sus vectores, perdemos la conexión léxica y semántica entre estas. Lo que no nos permite asegurar el significado que tenían estas palabras en el texto. 

Otro tema que me gustaría recalcar del artículo es cuando se menciona el tradeoff que existe entre la veracidad de los datos y la cantidad de estos. Esto ya que los métodos implícitos pueden recopilar una gran cantidad de datos con cierta incertidumbre sobre si al usuario realmente le gusta el artículo. Por el contrario, cuando el usuario califica explícitamente los elementos, hay poco o ningún ruido en los datos de entrenamiento, pero los usuarios tienden a proporcionar comentarios explícitos sobre solo un pequeño porcentaje de los elementos con los que interactúan. Entonces es desición del programador/empresa definir el limite entre la información que considera veraz y cual no. Además de los métodos que aplican para intentar eliminar la incertidumbre de los datos obtenidos.


Después el artículo menciona algoritmos existentes para realizar recomendaciones basadas en contenido. Algoritmos tales  como KNN, Árboles de decisión, lineales, Bernoulli y Bayesianos, los cuales ya vimos y discutimos en clases (por lo que no encontré sensato definirlos nuevamente). Lo que sí me gustaría recalcar es que depende mucho lo que queremos para escoger cual de estos algortimos ocupar. Por ejemplo los algoritmos lineales funcionan mucho mejor *on-line* debido a su simpleza y capacidad de actualizarse rápidamente. Por otro lado los árboles de decisión son super eficientes ppara cuando tenemos pocos atributos en nuestra base de datos. Entonces depende mucho de la información que tenemos y de nuestro objetivo para escoger el mejor algoritmo a utilizar. 

Finalmente quería concluir el comentario rescatando una cita del artículo:
 "Ningún sistema de recomendación basado en contenido puede ofrecer buenas recomendaciones si el contenido no contiene suficiente información para distinguir los elementos que le gustan al usuario de los elementos que no le gustan. "
Creo que esto es sumamente importante de entender, ya que podemos tener el mejor modelo existente hasta la fecha, pero si nuestra interfaz no motiva al usuario a generar contenido, nunca podremos darle buenas recomendaciones a este y por lo tanto no habremos cumplido con nuestro objetivo. 

